{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6cbe8de-217c-4d43-b767-13f2c983e147",
   "metadata": {},
   "source": [
    "# Chunking\n",
    "\n",
    "First off, the following code should not exist.\n",
    "\n",
    "```python\n",
    "# Now pop the first n_rollout steps from this walk and append them to the chunk\n",
    "for step in range(params['n_rollout']):\n",
    "    # For the first environment: simply copy the components (g, x, a) of each step\n",
    "    if len(chunk) < params['n_rollout']:\n",
    "        chunk.append([[comp] for comp in walk.pop(0)])\n",
    "    # For all next environments: add the components to the existing list of components for each step\n",
    "    else:\n",
    "        for comp_i, comp in enumerate(walk.pop(0)):\n",
    "            chunk[step][comp_i].append(comp)\n",
    "```\n",
    "\n",
    "What that is trying to do is something like so\n",
    "\n",
    "1. `walk.pop(0)` returns `[location, observation, action]`\n",
    "2. `[[comp] for comp in walk.pop(0)]` becomes `[[location], [observation], [action]]`\n",
    "\n",
    "When the chunk is has less than `n_rollout` entries it looks like so\n",
    "```\n",
    "chunk = [\n",
    "    [[loc1], [obs1], [act1]],\n",
    "    [[loc2], [obs2], [act2]],\n",
    "    ...\n",
    "]\n",
    "```\n",
    "\n",
    "**POSSIBLE BUG**\n",
    "```python\n",
    "# Now pop the first n_rollout steps from this walk and append them to the chunk\n",
    "for step in range(params['n_rollout']):\n",
    "    ...\n",
    "    # For all next environments: add the components to the existing list of components for each step\n",
    "    else:\n",
    "        for comp_i, comp in enumerate(walk.pop(0)):\n",
    "            chunk[step][comp_i].append(comp)\n",
    "```\n",
    "\n",
    "Not only cas its confusing but that should give an error because the code doesn't know that `chunk` is a list of lists.\n",
    "Switching to the `chunk[...][...]` syntax should raise an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "fc50e313-cc08-480d-a627-69414bab35b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "58949105-6537-4298-8497-1b614b1231d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./envs/5x5.json', 'r') as f:\n",
    "    env = json.load(f)\n",
    "\n",
    "for location in env['locations']:\n",
    "    location['shiny'] = None\n",
    "\n",
    "environments = [env]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e67b513f-e24b-4a0c-a9a4-8a741d949d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(env, walk):\n",
    "    \"\"\"\n",
    "    Each \"walk\" (step) is a list with the following:\n",
    "    1. new_location is the actual location from the env file.\n",
    "    2. new_observation is the one-hot-vector for the given observation.\n",
    "    3. new_action is the first action ID that is greater than some random number.\n",
    "    \"\"\"\n",
    "    # First step: start at random location\n",
    "    if len(walk) == 0:\n",
    "        new_location = np.random.randint(env['n_locations'])\n",
    "    # Any other step: get new location from previous location and action\n",
    "    else:\n",
    "        prev_location = walk[-1][0]\n",
    "        prev_action_chosen = walk[-1][2]\n",
    "        prev_location['actions'][prev_action_chosen]\n",
    "\n",
    "        # The transition array will have a 1 in the location we are moving to.\n",
    "        # The index with the 1, if there is 1, corresponds to an index in the\n",
    "        # locations array.\n",
    "        new_location = int(\n",
    "            np.flatnonzero(\n",
    "                np.cumsum(\n",
    "                    prev_location['actions'][prev_action_chosen]['transition'],\n",
    "                )>np.random.rand(),\n",
    "            )[0]\n",
    "        )\n",
    "    # Return the location dictionary of the new location.\n",
    "    return env['locations'][new_location]\n",
    "\n",
    "def get_observation(env, new_location):\n",
    "    # Find sensory observation for new state, and store it as one-hot vector\n",
    "    new_observation = np.eye(env['n_observations'])[new_location['observation']]\n",
    "    # Create a new observation by converting the new observation to a torch tensor\n",
    "    new_observation = torch.tensor(new_observation, dtype=torch.float).view((new_observation.shape[0]))\n",
    "    # Return the new observation\n",
    "    return new_observation\n",
    "\n",
    "def get_action(env, new_location, walk, repeat_bias_factor=2):\n",
    "    # Build policy from action probability of each action of provided location dictionary\n",
    "    policy = np.array([action['probability'] for action in new_location['actions']])  \n",
    "    \n",
    "    # Add a bias for repeating previous action to walk in straight lines, only if\n",
    "    # (this is not the first step) and (the previous action was a move)\n",
    "    policy[\n",
    "        [] if len(walk) == 0 or new_location['id'] == walk[-1][0]['id'] else walk[-1][2]\n",
    "    ] *= repeat_bias_factor\n",
    "    \n",
    "    # And renormalise policy (note that for unavailable actions, the policy was 0 and remains 0,\n",
    "    # so in that case no renormalisation needed)\n",
    "    policy = policy / sum(policy) if sum(policy) > 0 else policy\n",
    "    \n",
    "    # Select action in new state.\n",
    "    _some = np.random.rand()\n",
    "    new_action = int(np.flatnonzero(np.cumsum(policy)>_some)[0])\n",
    "    # Return the new action\n",
    "    return new_action\n",
    "\n",
    "def walk_default(env, walk, walk_length, repeat_bias_factor=2):\n",
    "    # Finish the provided walk until it contains walk_length steps\n",
    "    for curr_step in range(walk_length - len(walk)):\n",
    "        # Get new location based on previous action and location\n",
    "        new_location = get_location(env, walk)\n",
    "        # Get new observation at new location\n",
    "        new_observation = get_observation(env, new_location)\n",
    "        # Get new action based on policy at new location\n",
    "        new_action = get_action(env, new_location, walk)\n",
    "        # Append location, observation, and action to the walk\n",
    "        walk.append([new_location, new_observation, new_action])\n",
    "    # Return the final walk\n",
    "    return walk\n",
    "\n",
    "def generate_walks(env, walk_length=10, n_walk=100, repeat_bias_factor=2, shiny=False):\n",
    "    print(f'{walk_length=}')\n",
    "    print(f'{n_walk=}')\n",
    "    # Generate walk by sampling actions accoring to policy, then next state according to graph\n",
    "    walks = [] # This is going to contain a list of (state, observation, action) tuples\n",
    "    for currWalk in range(n_walk):\n",
    "        new_walk = []\n",
    "        # If shiny hasn't been specified: there are no shiny objects, generate default policy\n",
    "        if shiny is False:\n",
    "            new_walk = walk_default(env, new_walk, walk_length, repeat_bias_factor)\n",
    "        ## If shiny was specified: use policy that uses shiny policy to approach shiny objects\n",
    "        ## sequentially\n",
    "        ##else:\n",
    "        ##    new_walk = self.walk_shiny(new_walk, walk_length, repeat_bias_factor)\n",
    "        # Clean up walk a bit by only keep essential location dictionary entries\n",
    "        for step in new_walk[:-1]: # dont include the last step.\n",
    "            step[0] = {'id': step[0]['id'], 'shiny': step[0]['shiny']}\n",
    "        # Append new walk to list of walks\n",
    "        walks.append(new_walk)   \n",
    "    return walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e4d6c37d-3d39-45c3-a4f3-812bf6fc689d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "walk_length=14\n",
      "n_walk=1\n"
     ]
    }
   ],
   "source": [
    "# And make a single walk for each environment, where walk lengths can be any between the min and max\n",
    "# length to de-sychronise world switches.\n",
    "params = {\n",
    "    # Number of steps to roll out before backpropagation through time\n",
    "    'n_rollout': 2,\n",
    "    # Minimum length of a walk on one environment. Walk lengths are sampled uniformly\n",
    "    # from a window that shifts down until its lower limit is walk_it_min at the end of training\n",
    "    'walk_it_min': 5,\n",
    "    # Maximum length of a walk on one environment. Walk lengths are sampled uniformly from a window\n",
    "    # that starts with its upper limit at walk_it_max in the beginning of training, then shifts down\n",
    "    'walk_it_max': 8,\n",
    "}\n",
    "walks = [\n",
    "    generate_walks(\n",
    "        env,\n",
    "        params['n_rollout']*np.random.randint(params['walk_it_min'], params['walk_it_max']),\n",
    "        1,\n",
    "    )[0] for env in environments\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "886c7dc2-5beb-4a43-9ed5-6563da38ac01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, list)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(walks), type(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "86445c92-b44f-461d-b47f-165701d639be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, list)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(walks[0]), type(walks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8652f169-eff3-45b4-bb2e-4f5842ae5b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 5, 'shiny': None},\n",
       " tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 3]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walks[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a29f7190-8196-4df0-b9a5-eae2674641a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 10, 'shiny': None},\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " 3]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walks[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "743883b3-8ac1-45e1-9ac7-898f940f2de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 15, 'shiny': None},\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 3]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walks[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "e43b9acf-8293-44ce-b564-9b33a75958e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 11, 'shiny': None},\n",
       " tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 2]"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walks[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6b371e2f-31f2-45fa-83c9-23c5a5366d4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'id': 12,\n",
       "  'observation': 6,\n",
       "  'x': 0.5,\n",
       "  'y': 0.5,\n",
       "  'in_locations': [7, 11, 12, 13, 17],\n",
       "  'in_degree': 5,\n",
       "  'out_locations': [7, 11, 12, 13, 17],\n",
       "  'out_degree': 5,\n",
       "  'actions': [{'id': 0,\n",
       "    'transition': [0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     1,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    'probability': 0.2},\n",
       "   {'id': 1,\n",
       "    'transition': [0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     1,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    'probability': 0.2},\n",
       "   {'id': 2,\n",
       "    'transition': [0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     1,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    'probability': 0.2},\n",
       "   {'id': 3,\n",
       "    'transition': [0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     1,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    'probability': 0.2},\n",
       "   {'id': 4,\n",
       "    'transition': [0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     1,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0],\n",
       "    'probability': 0.2}],\n",
       "  'shiny': None},\n",
       " tensor([0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 1]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walks[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "12123f31-4598-42e0-a70e-f8a92751d821",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of walks to generate\n",
    "params['train_it'] = 5\n",
    "\n",
    "# Train TEM on walks in different environment\n",
    "for i in range(0, params['train_it']):\n",
    "    # Make an empty chunk that will be fed to TEM in this backprop iteration\n",
    "    chunk = []\n",
    "    # For each environment: fill chunk by popping the first batch_size steps of the walk\n",
    "    for env_i, walk in enumerate(walks):\n",
    "        # Now pop the first n_rollout steps from this walk and append them to the chunk\n",
    "        for step in range(params['n_rollout']):\n",
    "            # For the first environment: simply copy the components (g, x, a) of each step\n",
    "            if len(chunk) < params['n_rollout']:\n",
    "                chunk.append([[comp] for comp in walk.pop(0)])\n",
    "            # For all next environments: add the components to the existing list of components for each step\n",
    "            else:\n",
    "                for comp_i, comp in enumerate(walk.pop(0)):\n",
    "                    chunk[step][comp_i].append(comp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "df205f14-82ac-4a0c-8c11-87bc25d4135c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[[{'id': 15, 'shiny': None}],\n",
       "   [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "   [0]],\n",
       "  [[{'id': 15, 'shiny': None}],\n",
       "   [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "            0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "   [2]]],\n",
       " 2,\n",
       " list)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk, len(chunk), type(chunk) # chunk.append([[comp] for comp in walk.pop(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "67eeddef-fdf0-449f-8d03-f9fe5971d883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[{'id': 15, 'shiny': None}],\n",
       "  [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.])],\n",
       "  [0]],\n",
       " 3,\n",
       " list)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0], len(chunk[0]), type(chunk[0]) # [[comp] for comp in walk.pop(0)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "753bb579-f879-4e58-9228-c060b7903599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'id': 15, 'shiny': None}], 1, list)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0][0], len(chunk[0][0]), type(chunk[0][0]) # [chunk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "850a5426-7d22-4ea8-8bb8-c82208f36a15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 15, 'shiny': None}, 2, dict)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0][0][0], len(chunk[0][0][0]), type(chunk[0][0][0]) # chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "8dcfbabf-10f8-4ab1-aef5-d22138b8b508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[{'id': 15, 'shiny': None}], [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])], [0]]\n",
      "[[{'id': 15, 'shiny': None}], [tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0.])], [2]]\n"
     ]
    }
   ],
   "source": [
    "# NOTE: this step doesn't do anything in this setup.\n",
    "# TODO: check that it actually has a purpose.\n",
    "\n",
    "# Stack all observations (x, component 1) into tensors along the first dimension for batch processing\n",
    "for i_step, step in enumerate(chunk):\n",
    "    print(f'{step}')\n",
    "    chunk[i_step][1] = torch.stack(step[1], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "79b4d4ee-ad93-4bcc-8ba8-ec6667ab740c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'id': 15, 'shiny': None}, 2, dict)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0][0][0], len(chunk[0][0][0]), type(chunk[0][0][0]) # chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "10662eba-2f99-4cbf-a248-690cdeeb1e5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0., 0., 0., 0., 0., 0., 0., 0., 0.]),\n",
       " 45,\n",
       " torch.Tensor)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk[0][1][0], len(chunk[0][1][0]), type(chunk[0][1][0]) # chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "eefd5315-14f2-40aa-9c71-cc8bbb155ddd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[[{'id': 15, 'shiny': None}],\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [0]],\n",
       " [[{'id': 15, 'shiny': None}],\n",
       "  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "           0., 0., 0., 0., 0., 0., 0., 0., 0.]]),\n",
       "  [2]]]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e852522-4e35-4ab4-8f77-2aa09968417e",
   "metadata": {},
   "source": [
    "# Making sense of the damn model\n",
    "\n",
    "The TEM has two main parts:\n",
    "\n",
    "**Grid cells (g)** - learn abstract spatial structure that generalizes across environments\n",
    "**Place cells (p)** - bind specific sensory experiences to locations in the current environment\n",
    "\n",
    "Key Functions to Understand:\n",
    "\n",
    "1. `gen_g()` - Does \"path integration\" (moves grid cells based on action)\n",
    "1. `inference()` - Figures out current place cell state from sensory input + grid cells\n",
    "1. `generative()` - Predicts what observation should be seen given current state\n",
    "1. `hebbian()` - Updates memory matrix M\n",
    "\n",
    "MLP is just a simple Multi-Layer Perceptron (basic neural network).\n",
    "It's a utility class that creates a 2-layer network: input > hidden > output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7117c40-6d2d-4a45-a66d-5621798bfadb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
