{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cb7658-417e-4f3b-a3a6-62e3fa5368cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546fa153-d208-474f-9952-b3ea86fc52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db91c76-4bd3-49c0-b72e-d17c622d6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0722639-b3ac-4fe5-b019-136177c0c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20_479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would ha\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of character: {len(raw_text):_}\")\n",
    "print(raw_text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14bd59-91ea-4dad-9e06-55f2de5f59b7",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8651cb62-88dc-4eae-8cd7-632f7f758ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95233c7e-6a9f-4bca-af51-104806750e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# pympler will give you a more accurate picture since it recursively measures all\n",
    "# the memory used by the object and its contents.\n",
    "from pympler import asizeof\n",
    "\n",
    "def cmp_sizes(obj):\n",
    "    sys_size_mb =  sys.getsizeof(obj) / 1024 / 1024\n",
    "    pympler_size_mb = asizeof.asizeof(obj) / 1024 / 1024\n",
    "    print(f\"Total size according to sys: {sys_size_mb:.5f} MB\")\n",
    "    print(f\"Total size according to pympler: {pympler_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7cfcafe1-53f0-4f29-b1fa-c447b9b72bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.00005 MB\n",
      "Total size according to pympler: 6.08 MB\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "cmp_sizes(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d222ab2-270b-4481-aac8-641eee532a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.00005 MB\n",
      "Total size according to pympler: 12.13 MB\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "cmp_sizes(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d943faa-871e-4815-b305-8ef8496c52a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.00005 MB\n",
      "Total size according to pympler: 24.31 MB\n"
     ]
    }
   ],
   "source": [
    "cmp_sizes(tiktoken.get_encoding('o200k_base'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "193d33ca-7707-452e-95e6-a34b60bd91d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.01957 MB\n",
      "Total size according to pympler: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "cmp_sizes(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "32965c5b-0b79-4a42-94c3-6efca490e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.03777 MB\n",
      "Total size according to pympler: 0.17 MB\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "cmp_sizes(enc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0816cf-3c11-43e9-bdc3-b90def2c9176",
   "metadata": {},
   "source": [
    "# Input-Target Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "affc4dd8-f2e3-45fa-80b5-966c0a5ac7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f8ead0d-ecac-49fe-b93b-24f90a6cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[40, 473, 1846, 2744]\n",
      "    y=[473, 1846, 2744, 3463]\n"
     ]
    }
   ],
   "source": [
    "x = enc_text[: ctx_size]\n",
    "y = enc_text[1: ctx_size+1]\n",
    "\n",
    "print(f'{x=}')\n",
    "print(f'    {y=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1215abe-3533-478f-a897-e0b1fa756dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] --> 473\n",
      "\tI -->  H\n",
      "[40, 473] --> 1846\n",
      "\tI H --> AD\n",
      "[40, 473, 1846] --> 2744\n",
      "\tI HAD -->  always\n",
      "[40, 473, 1846, 2744] --> 3463\n",
      "\tI HAD always -->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, ctx_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "    print(f'{context} --> {desired}')\n",
    "    print(f'\\t{tokenizer.decode(context)} --> {tokenizer.decode([desired])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5597b-ec24-4aa4-9d80-c9978f329898",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "de7da98c-95ef-44f0-86ed-1cbe7caa410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af201041-679c-4288-b47a-fa5931a8d2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "670e3b89-c285-4c8b-9cc8-20c5fb664c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO AVX'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cpu.get_cpu_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed0d961b-a3c6-4731-8fce-80e111515288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    _device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    _device = 'mps'\n",
    "else:\n",
    "    _device = 'cpu'\n",
    "\n",
    "device = torch.device(_device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2ccdc27-093b-4da4-b426-4b5e98629652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    # detach() detaches it from the current graph - result will never require gradient.\n",
    "    # cpu() copies it to the CPU.\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7a340-8983-4a94-867b-60f667868ee2",
   "metadata": {},
   "source": [
    "## Pytorch Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f700e17-f1da-4418-bad6-635f1fb0db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "107da34e-0a2e-4915-9c28-a83a5489391c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "iinfo(min=-2.14748e+09, max=2.14748e+09, dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Types https://pytorch.org/docs/stable/tensors.html\n",
    "torch.iinfo(torch.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "04b68bd3-602c-448c-b662-7ea9c6feb7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_type_range(dtype):\n",
    "    info = torch.iinfo(dtype)\n",
    "    print(f\"{dtype}: {info.min:_d} to {info.max:_d}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4b956680-67e9-4049-8849-8f7bd11b3da0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.int16: -32_768 to 32_767\n",
      "torch.int32: -2_147_483_648 to 2_147_483_647\n",
      "torch.int64: -9_223_372_036_854_775_808 to 9_223_372_036_854_775_807\n"
     ]
    }
   ],
   "source": [
    "print_type_range(torch.short)\n",
    "print_type_range(torch.int)\n",
    "print_type_range(torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88c9ba03-3fa6-4c5b-b2a3-78a9e5280102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to make sense of the sliding window for-loop below.\n",
    "# Takeaway: range is inclusive on the left, exclusive on the right.\n",
    "[i for i in range(0, 10, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25db7c88-0d18-490a-a102-13f4371efb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 5), (2, 6), (3, 7), (4, 8), (5, 9)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to make sense of the sliding window for-loop below.\n",
    "# Takeaway: if we know the total number of token IDs, we can create\n",
    "# len(token IDs) - max_length sequences of size max_lenght at most (if stride is 1).\n",
    "_len_token_ids = 10\n",
    "_max_length = 4      # This is the lenght of the seuquences we want to create\n",
    "_stride = 1\n",
    "\n",
    "\n",
    "[(i, i+_max_length) for i in range(0, _len_token_ids - _max_length, _stride)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca0b0e84-0cef-46fb-8d7a-da29686d549d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4943"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(raw_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dbb7bc88-2c2f-4b67-9117-6178ab3090de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenize the entire text.\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i+max_length]\n",
    "            target_chunk = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Return the total number of rows in the dataset\n",
    "        \"\"\"\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Return row idx from the dataset\n",
    "        \"\"\"\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "\n",
    "def create_dataloader_v1(\n",
    "    txt, /, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0,\n",
    "    encoding='cl100k_base',\n",
    "):\n",
    "    # Initialize the tokenizer.\n",
    "    tokenizer = tiktoken.get_encoding(encoding)\n",
    "\n",
    "    # Create dataset.\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65a28537-7cb3-4e3d-bf90-a3e35111b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40,  473, 1846, 2744]]), tensor([[ 473, 1846, 2744, 3463]])]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batches of size max_lenght.\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71cb590f-7f09-47b7-a2c6-1f9d82e0b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx=0\n",
      "\tfeatures=tensor([[  40,  473, 1846, 2744]], device='mps:0')\n",
      "\tlabels=tensor([[ 473, 1846, 2744, 3463]], device='mps:0')\n",
      "batch_idx=1\n",
      "\tfeatures=tensor([[ 473, 1846, 2744, 3463]], device='mps:0')\n",
      "\tlabels=tensor([[1846, 2744, 3463, 7762]], device='mps:0')\n",
      "batch_idx=2\n",
      "\tfeatures=tensor([[1846, 2744, 3463, 7762]], device='mps:0')\n",
      "\tlabels=tensor([[2744, 3463, 7762,  480]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    print(f'{batch_idx=}\\n\\t{features=}\\n\\t{labels=}')\n",
    "\n",
    "    if batch_idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e3fcc85b-52b0-42d2-a22a-2c41efc80e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx=0\n",
      "  features=tensor([[  40,  473, 1846, 2744],\n",
      "        [ 473, 1846, 2744, 3463]], device='mps:0')\n",
      "  labels=tensor([[ 473, 1846, 2744, 3463],\n",
      "        [1846, 2744, 3463, 7762]], device='mps:0')\n",
      "batch_idx=1\n",
      "  features=tensor([[1846, 2744, 3463, 7762],\n",
      "        [2744, 3463, 7762,  480]], device='mps:0')\n",
      "  labels=tensor([[2744, 3463, 7762,  480],\n",
      "        [3463, 7762,  480,  285]], device='mps:0')\n",
      "batch_idx=2\n",
      "  features=tensor([[ 3463,  7762,   480,   285],\n",
      "        [ 7762,   480,   285, 22464]], device='mps:0')\n",
      "  labels=tensor([[ 7762,   480,   285, 22464],\n",
      "        [  480,   285, 22464,  4856]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Changed the batch size.\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    print(f'{batch_idx=}\\n  {features=}\\n  {labels=}')\n",
    "\n",
    "    if batch_idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c42f62e-6fdb-4332-bdf5-4375702e72e2",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "* Smaller batches fit beter in memory but they may lead to noisier model updates\n",
    "* overlapping chunks (stride smaller than max lenght) may cause overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8cf0a5-f6f3-4640-82a7-bd1a67cb839e",
   "metadata": {},
   "source": [
    "### Digression: multicolinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "733abb76-8f94-4b56-a38c-7c9c6ba81641",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b5d8013a-ab77-4bad-b536-d17c4ee807de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OneHotEncoder will always produce multicollinearity unless you specifically tell it not to.\n",
    "# For any categorical variable with n categories, OneHotEncoder by default creates n binary columns where:\n",
    "# \n",
    "# Each row must have exactly one 1 and the rest 0s\n",
    "# Therefore, the sum across any row must = 1\n",
    "# This means if you know n-1 columns, you can perfectly predict the nth column\n",
    "#\n",
    "# You can avoid this by:\n",
    "#\n",
    "# Using drop='first' parameter in OneHotEncoder to drop the first column\n",
    "# Or using drop='if_binary' which drops a column only when encoding binary features\n",
    "color_ohe = OneHotEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "faa74823-b544-455a-bb82-c7e1a777575f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['red'],\n",
       "       ['blue'],\n",
       "       ['green']], dtype='<U5')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors = np.array(['red', 'blue', 'green'])\n",
    "colors = colors.reshape(-1, 1)\n",
    "colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1b233eea-6dca-4d92-a96e-eb8642cefd8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_matrix = color_ohe.fit_transform(colors).toarray()\n",
    "ohe_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7b120edc-2697-4f92-bef2-4b23e2cd1279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1. , -0.5, -0.5],\n",
       "       [-0.5,  1. , -0.5],\n",
       "       [-0.5, -0.5,  1. ]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use the transpose to correlate columns against columns.\n",
    "# Otherwise you'll correlate rows, which is not what you want.\n",
    "#\n",
    "# The diagonal is always 1.0 (a variable perfectly correlates with itself).\n",
    "# The off-diagonal values of -0.5 show negative correlations between different colors.\n",
    "# When one color is present (1), both other colors must be absent (0).\n",
    "# This creates a negative relationship: knowing one color is present tells you the others must be absent.\n",
    "# The -0.5 specifically comes from the balanced nature of your data (equal numbers of each color).\n",
    "np.corrcoef(ohe_matrix.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "020385b2-3628-4e74-b6e1-0855d00438f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['red'],\n",
       "       ['blue'],\n",
       "       ['green'],\n",
       "       ['red'],\n",
       "       ['green']], dtype='<U5')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "colors_v2 = np.array(['red', 'blue', 'green', 'red', 'green']).reshape(-1, 1)\n",
    "colors_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3ee01e76-e855-4d03-a44f-f2d64d152a2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1.],\n",
       "       [1., 0., 0.],\n",
       "       [0., 1., 0.],\n",
       "       [0., 0., 1.],\n",
       "       [0., 1., 0.]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ohe_matrix_v2 = color_ohe.fit_transform(colors_v2).toarray()\n",
    "ohe_matrix_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "771960ad-116f-4f42-9d09-3f506629897e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.40824829, -0.40824829],\n",
       "       [-0.40824829,  1.        , -0.66666667],\n",
       "       [-0.40824829, -0.66666667,  1.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(ohe_matrix_v2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "39b257d2-4373-4863-bd40-2752f401b760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        , -0.31622777, -0.4472136 ],\n",
       "       [-0.31622777,  1.        , -0.70710678],\n",
       "       [-0.4472136 , -0.70710678,  1.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Correlation of red vs. blue is less than red vs. green.\n",
    "# Red is more frequent, followed by green.\n",
    "# Correlation of blue vs. green is even more negative.\n",
    "colors_v3 = np.array(['red', 'blue', 'green', 'red', 'green', 'red']).reshape(-1, 1)\n",
    "ohe_matrix_v3 = color_ohe.fit_transform(colors_v3).toarray()\n",
    "np.corrcoef(ohe_matrix_v3.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e0449078-da86-4ac8-9d6e-a5f983672544",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(3)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.matrix_rank(ohe_matrix_v2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befedecc-98ca-4abc-9b4e-e040a738cb30",
   "metadata": {},
   "source": [
    "# Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6033331a-0422-4b81-80c6-20d3920791d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_input_ids = torch.tensor([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e57e26e7-736a-4401-a504-87126e752da8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374, -0.1778, -0.1690],\n",
       "        [ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], requires_grad=True)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 6\n",
    "output_dim = 3\n",
    "\n",
    "torch.manual_seed(123)\n",
    "\n",
    "embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "embedding_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "265ea3b5-5323-4d8b-8a59-b9764addc67b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4015,  0.9666, -1.1481]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer( torch.tensor([3]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "897a1d67-aa64-4dfe-a9b0-5b290a796efa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer( torch.tensor([3]) ) == embedding_layer.weight[3, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2b9fcb09-7c64-4541-bf02-f0bb7a35f982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer( torch.tensor([1]) ) == embedding_layer.weight[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c167a79f-efa7-4a15-b22f-a442947b00b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer( dummy_input_ids )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e6c3151d-f1c4-4092-ae32-2e42a5c8318f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 1, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1, 0],\n",
       "        [0, 0, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.nn.functional.one_hot( dummy_input_ids )\n",
    "onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dcc727df-a846-4f2a-8a0e-cdd849792e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1665,  0.0135, -0.2028,  0.1540, -0.3479,  0.2993],\n",
       "        [-0.2967, -0.3246, -0.2580,  0.1849, -0.1508,  0.1528],\n",
       "        [-0.3465, -0.2477, -0.1499, -0.0802, -0.3114,  0.2673]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(123) # and YES! we DO need this again!\n",
    "\n",
    "linear = torch.nn.Linear(vocab_size, output_dim, bias=False)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d95162ef-8416-474e-b23b-c67441da55e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.3374,  0.9178,  1.2753, -0.4015, -1.1589, -2.8400],\n",
       "        [-0.1778,  1.5810, -0.2010,  0.9666,  0.3255, -0.7849],\n",
       "        [-0.1690,  1.3010, -0.1606, -1.1481, -0.6315, -1.4096]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear.weight = torch.nn.Parameter(embedding_layer.weight.T)\n",
    "linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9a55e596-9d5f-4dbc-b5f8-f1ac3bbfe98b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.9178,  1.5810,  1.3010],\n",
       "        [ 1.2753, -0.2010, -0.1606],\n",
       "        [-0.4015,  0.9666, -1.1481],\n",
       "        [-1.1589,  0.3255, -0.6315],\n",
       "        [-2.8400, -0.7849, -1.4096]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Matrix multiplication XW^T.\n",
    "# The matmul by the one-got-encoding is \"just\" a multiplication by the identity.\n",
    "# So we get the same embedding.\n",
    "# But the thing to keep in mindis that the embedding was initialized with random weights,\n",
    "# so the embedding is just a different way to do a OneHotEncoding - prefered because this is now\n",
    "# a \"normal\" layer that can be optimized via backpropagation.\n",
    "linear(onehot.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f1472-a41f-495c-ab8c-b22b8f55e737",
   "metadata": {},
   "source": [
    "## Positional Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "00ebad58-d1be-41db-ad2b-16b0740aa636",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100277"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.n_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "821f2808-cad0-4655-90c9-e9e4f9357ed8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(100277, 256)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = tokenizer.n_vocab\n",
    "output_dim = 256 # Size of our embeddings.\n",
    "\n",
    "token_embedding_layer = torch.nn.Embedding(vocab_size, output_dim)\n",
    "token_embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e9f3bfc-f73d-481a-b04f-621f3f3b323c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_length = 4\n",
    "dataloader = create_dataloader_v1(\n",
    "    raw_text,\n",
    "    batch_size=8,\n",
    "    max_length=max_length,\n",
    "    stride=max_length,\n",
    "    shuffle=False\n",
    ")\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ebdd115-a49d-4aef-a8ec-e46f95118e71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embeddings = token_embedding_layer(inputs)\n",
    "token_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7e62bb78-6e59-4a08-9287-8ea88b700cd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 256])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_length = max_length\n",
    "pos_embedding_layer = torch.nn.Embedding(context_length, output_dim)\n",
    "pos_embeddings = pos_embedding_layer(torch.arange(max_length))\n",
    "pos_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "443adc2a-8ed4-456e-82bd-7a356a4b8c17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 4, 256])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_embeddings = token_embeddings + pos_embeddings\n",
    "input_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c058a87f-721f-476e-bb2b-1a1d9362bc7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
