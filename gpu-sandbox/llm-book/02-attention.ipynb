{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4eeb520-517a-47d8-afc8-cd3b68c0f072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "408549c3-0d5c-4832-befd-c5c2b2ab4b37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.7.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7df055d6-0ae4-42c7-995b-85d9eb8f7777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4300, 0.1500, 0.8900],\n",
       "        [0.5500, 0.8700, 0.6600],\n",
       "        [0.5700, 0.8500, 0.6400],\n",
       "        [0.2200, 0.5800, 0.3300],\n",
       "        [0.7700, 0.2500, 0.1000],\n",
       "        [0.0500, 0.8000, 0.5500]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The input sequence has already been embeded into 3D vectors.\n",
    "inputs = torch.tensor(\n",
    "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
    "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
    "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
    "   [0.22, 0.58, 0.33], # with     (x^4)\n",
    "   [0.77, 0.25, 0.10], # one      (x^5)\n",
    "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
    ")\n",
    "inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90c89d9-3997-4696-807b-7a8d6dbc7184",
   "metadata": {},
   "source": [
    "## Attention Scores and Weights for a Single Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24fc40aa-1e61-40c0-ae48-28d4b4cf07bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.5500, 0.8700, 0.6600]), torch.Size([3]), torch.Size([6, 3]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = inputs[1]  # 2nd input token is the query\n",
    "query, query.shape, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76fb2098-a749-46d1-a342-c03c9cbc112c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i=tensor([0.4300, 0.1500, 0.8900])\n",
      "x_i=tensor([0.5500, 0.8700, 0.6600])\n",
      "x_i=tensor([0.5700, 0.8500, 0.6400])\n",
      "x_i=tensor([0.2200, 0.5800, 0.3300])\n",
      "x_i=tensor([0.7700, 0.2500, 0.1000])\n",
      "x_i=tensor([0.0500, 0.8000, 0.5500])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs): # iterates through the rows.\n",
    "    # dot product (transpose not necessary here since they are 1-dim vectors)\n",
    "    print(f'{x_i=}')\n",
    "    attn_scores_2[i] = torch.dot(x_i, query)\n",
    "\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92971d05-d079-404f-b45f-bf49ea0af692",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same but with the transpose in the right place just to get into the habit.\n",
    "attn_scores_2 = torch.empty(inputs.shape[0])\n",
    "for i, x_i in enumerate(inputs):\n",
    "    attn_scores_2[i] = torch.dot(x_i.T, query)\n",
    "\n",
    "attn_scores_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c13c290a-07de-4413-bf84-a4b3482a1833",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same but in one-go.\n",
    "inputs @ query  # 6x3 x 3x1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "171cc7c0-fdfd-4f45-8950-df4372c53c2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656]), tensor(1.0000))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Obtain attention weights by normalizing attention scores.\n",
    "attn_weights_2_tmp = attn_scores_2 / attn_scores_2.sum()\n",
    "attn_weights_2_tmp, attn_weights_2_tmp.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1495f29c-e0e5-4d38-b9e1-fc62e289f64c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581]), tensor(1.))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Softmax is better at handling extreme values and has more desirable gradient properties during training.\n",
    "attn_weights_2 = torch.softmax(attn_scores_2, dim=0)\n",
    "attn_weights_2, attn_weights_2.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cefd440f-128e-4cd1-99f3-defb6eda7897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The efficient way of doing it.\n",
    "_attn_scores_2 = inputs @ query\n",
    "torch.softmax( _attn_scores_2, dim=0 ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bf7915-916d-433f-8242-4b6679066f9b",
   "metadata": {},
   "source": [
    "## A single Context Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a0222e1b-5d94-4878-95c4-af69195bfcca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "469156f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.4419, 0.6515, 0.5683])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vec_2 = torch.zeros(query.shape)\n",
    "for i, x_i in enumerate(inputs):\n",
    "    context_vec_2 += attn_weights_2[i]*x_i\n",
    "\n",
    "context_vec_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "378e1ea9-487c-4077-9195-090265e87331",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([6]), torch.Size([3]), torch.Size([6, 3]))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_weights_2.shape, inputs[0].shape, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38bec1e9-33dd-49f7-b001-3f3406932584",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.1385],\n",
       "         [0.2379],\n",
       "         [0.2333],\n",
       "         [0.1240],\n",
       "         [0.1082],\n",
       "         [0.1581]]),\n",
       " torch.Size([6, 1]),\n",
       " torch.Size([6, 3]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://pytorch.org/docs/stable/generated/torch.unsqueeze.html\n",
    "attn_weights_2.unsqueeze(-1), attn_weights_2.unsqueeze(-1).shape, inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c37762e5-0112-42b4-8dff-d0d6a7a84115",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4419, 0.6515, 0.5683]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same operation as above but in one-go.\n",
    "attn_weights_2.unsqueeze(-1).T @ inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dad3b2-9de8-4163-b52d-d25031547d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
