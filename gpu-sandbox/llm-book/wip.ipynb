{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45cb7658-417e-4f3b-a3a6-62e3fa5368cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546fa153-d208-474f-9952-b3ea86fc52de",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"the-verdict.txt\"):\n",
    "    url = (\"https://raw.githubusercontent.com/rasbt/\"\n",
    "           \"LLMs-from-scratch/main/ch02/01_main-chapter-code/\"\n",
    "           \"the-verdict.txt\")\n",
    "    file_path = \"the-verdict.txt\"\n",
    "    urllib.request.urlretrieve(url, file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1db91c76-4bd3-49c0-b72e-d17c622d6775",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0722639-b3ac-4fe5-b019-136177c0c7ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20_479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no great surprise to me to hear that, in the height of his glory, he had dropped his painting, married a rich widow, and established himself in a villa on the Riviera. (Though I rather thought it would ha\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total number of character: {len(raw_text):_}\")\n",
    "print(raw_text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de14bd59-91ea-4dad-9e06-55f2de5f59b7",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8651cb62-88dc-4eae-8cd7-632f7f758ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95233c7e-6a9f-4bca-af51-104806750e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "# pympler will give you a more accurate picture since it recursively measures all\n",
    "# the memory used by the object and its contents.\n",
    "from pympler import asizeof\n",
    "\n",
    "def cmp_sizes(obj):\n",
    "    sys_size_mb =  sys.getsizeof(obj) / 1024 / 1024\n",
    "    pympler_size_mb = asizeof.asizeof(obj) / 1024 / 1024\n",
    "    print(f\"Total size according to sys: {sys_size_mb:.5f} MB\")\n",
    "    print(f\"Total size according to pympler: {pympler_size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7cfcafe1-53f0-4f29-b1fa-c447b9b72bd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.00005 MB\n",
      "Total size according to pympler: 6.08 MB\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "cmp_sizes(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d222ab2-270b-4481-aac8-641eee532a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.00005 MB\n",
      "Total size according to pympler: 12.13 MB\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tiktoken.get_encoding('cl100k_base')\n",
    "cmp_sizes(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "193d33ca-7707-452e-95e6-a34b60bd91d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.01957 MB\n",
      "Total size according to pympler: 0.02 MB\n"
     ]
    }
   ],
   "source": [
    "cmp_sizes(raw_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "32965c5b-0b79-4a42-94c3-6efca490e147",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total size according to sys: 0.03777 MB\n",
      "Total size according to pympler: 0.17 MB\n"
     ]
    }
   ],
   "source": [
    "enc_text = tokenizer.encode(raw_text)\n",
    "cmp_sizes(enc_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef0816cf-3c11-43e9-bdc3-b90def2c9176",
   "metadata": {},
   "source": [
    "# Input-Target Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "affc4dd8-f2e3-45fa-80b5-966c0a5ac7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx_size = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f8ead0d-ecac-49fe-b93b-24f90a6cf40a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=[40, 473, 1846, 2744]\n",
      "    y=[473, 1846, 2744, 3463]\n"
     ]
    }
   ],
   "source": [
    "x = enc_text[: ctx_size]\n",
    "y = enc_text[1: ctx_size+1]\n",
    "\n",
    "print(f'{x=}')\n",
    "print(f'    {y=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f1215abe-3533-478f-a897-e0b1fa756dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[40] --> 473\n",
      "I -->  H\n",
      "[40, 473] --> 1846\n",
      "I H --> AD\n",
      "[40, 473, 1846] --> 2744\n",
      "I HAD -->  always\n",
      "[40, 473, 1846, 2744] --> 3463\n",
      "I HAD always -->  thought\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, ctx_size+1):\n",
    "    context = enc_text[:i]\n",
    "    desired = enc_text[i]\n",
    "    print(f'{context} --> {desired}')\n",
    "    print(f'{tokenizer.decode(context)} --> {tokenizer.decode([desired])}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb5597b-ec24-4aa4-9d80-c9978f329898",
   "metadata": {},
   "source": [
    "# Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de7da98c-95ef-44f0-86ed-1cbe7caa410d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af201041-679c-4288-b47a-fa5931a8d2a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.mps.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "670e3b89-c285-4c8b-9cc8-20c5fb664c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NO AVX'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.backends.cpu.get_cpu_capability()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed0d961b-a3c6-4731-8fce-80e111515288",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mps'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    _device = 'cuda'\n",
    "elif torch.backends.mps.is_available():\n",
    "    _device = 'mps'\n",
    "else:\n",
    "    _device = 'cpu'\n",
    "\n",
    "_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "323ddadb-49a6-48e8-a9c0-d75cbb9f937e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(_device)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b2ccdc27-093b-4da4-b426-4b5e98629652",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_numpy(tensor):\n",
    "    # cpu() copies it to the CPU.\n",
    "    # detach() detaches it from the current graph - result will never require gradient.\n",
    "    return tensor.detach().cpu().numpy() if tensor.requires_grad else tensor.cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7a340-8983-4a94-867b-60f667868ee2",
   "metadata": {},
   "source": [
    "## Pytorch Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0f700e17-f1da-4418-bad6-635f1fb0db3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "88c9ba03-3fa6-4c5b-b2a3-78a9e5280102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to make sense of the sliding window for-loop below.\n",
    "[i for i in range(0, 10, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dbb7bc88-2c2f-4b67-9117-6178ab3090de",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "        \n",
    "        # Tokenize the entire text.\n",
    "        token_ids = tokenizer.encode(txt)\n",
    "\n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length.\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i : i+max_length]\n",
    "            target_chunk = token_ids[i+1 : i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk, dtype=torch.float32))\n",
    "            self.target_ids.append(torch.tensor(target_chunk, dtype=torch.float32))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]\n",
    "\n",
    "def create_dataloader_v1(\n",
    "    txt, /, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0,\n",
    "    encoding='cl100k_base',\n",
    "):\n",
    "    # Initialize the tokenizer.\n",
    "    tokenizer = tiktoken.get_encoding(encoding)\n",
    "\n",
    "    # Create dataset.\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader.\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25db7c88-0d18-490a-a102-13f4371efb29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 4), (1, 5), (2, 6), (3, 7), (4, 8), (5, 9)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example to make sense of the sliding window for-loop below.\n",
    "_len_token_ids = 10\n",
    "_max_length = 4\n",
    "_stride = 1\n",
    "[(i, i+_max_length) for i in range(0, _len_token_ids - _max_length, _stride)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "65a28537-7cb3-4e3d-bf90-a3e35111b03f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[  40.,  473., 1846., 2744.]]),\n",
       " tensor([[ 473., 1846., 2744., 3463.]])]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create batches of size max_lenght.\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "first_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "71cb590f-7f09-47b7-a2c6-1f9d82e0b14e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx=0\n",
      "\tfeatures=tensor([[  40.,  473., 1846., 2744.]], device='mps:0')\n",
      "\tlabels=tensor([[ 473., 1846., 2744., 3463.]], device='mps:0')\n",
      "batch_idx=1\n",
      "\tfeatures=tensor([[ 473., 1846., 2744., 3463.]], device='mps:0')\n",
      "\tlabels=tensor([[1846., 2744., 3463., 7762.]], device='mps:0')\n",
      "batch_idx=2\n",
      "\tfeatures=tensor([[1846., 2744., 3463., 7762.]], device='mps:0')\n",
      "\tlabels=tensor([[2744., 3463., 7762.,  480.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    print(f'{batch_idx=}\\n\\t{features=}\\n\\t{labels=}')\n",
    "\n",
    "    if batch_idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3fcc85b-52b0-42d2-a22a-2c41efc80e4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_idx=0\n",
      "  features=tensor([[  40.,  473., 1846., 2744.],\n",
      "        [ 473., 1846., 2744., 3463.]], device='mps:0')\n",
      "  labels=tensor([[ 473., 1846., 2744., 3463.],\n",
      "        [1846., 2744., 3463., 7762.]], device='mps:0')\n",
      "batch_idx=1\n",
      "  features=tensor([[1846., 2744., 3463., 7762.],\n",
      "        [2744., 3463., 7762.,  480.]], device='mps:0')\n",
      "  labels=tensor([[2744., 3463., 7762.,  480.],\n",
      "        [3463., 7762.,  480.,  285.]], device='mps:0')\n",
      "batch_idx=2\n",
      "  features=tensor([[ 3463.,  7762.,   480.,   285.],\n",
      "        [ 7762.,   480.,   285., 22464.]], device='mps:0')\n",
      "  labels=tensor([[ 7762.,   480.,   285., 22464.],\n",
      "        [  480.,   285., 22464.,  4856.]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "# Changed the batch size.\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=2, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "for batch_idx, (features, labels) in enumerate(dataloader):\n",
    "    features, labels = features.to(device), labels.to(device)\n",
    "    print(f'{batch_idx=}\\n  {features=}\\n  {labels=}')\n",
    "\n",
    "    if batch_idx > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "733abb76-8f94-4b56-a38c-7c9c6ba81641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
