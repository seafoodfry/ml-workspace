{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cdf889e6-b697-4d57-8760-7f72316a41a5",
   "metadata": {},
   "source": [
    "[Multi-digit Number Recognition from Street View Imagery using Deep Convolutional Neural Networks](https://arxiv.org/abs/1312.6082)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8398206c-9da4-48c2-8b49-d897418fdd07",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "> Like most neural networks, they contain several filtering layers\n",
    "with each layer applying an affine transformation to the vector input followed by an elementwise\n",
    "non-linearity.\n",
    "\n",
    "an affine map is the composition of two functions: a translation and a linear map\n",
    "$$\n",
    "\\vec{y} = f(\\vec{x}) = A\\vec{x} + \\vec{b}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "y \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "=\n",
    "\\begin{bmatrix}\n",
    "A & b \\\\\n",
    "0 & 1\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix}\n",
    "x \\\\\n",
    "1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "This happens when the input (i.e., an image patch) is multiplied by weights (filter/kernel) and then a bias is added.\n",
    "This affine transformation preserves lines and parallelism but not necessarily angles or distances.\n",
    "\n",
    "\n",
    "After the affine transformation, each resulting value goes through a non-linear function\n",
    "(like ReLU, sigmoid, or tanh) independently.\n",
    "\"Elementwise\" means it's applied to each value separately, not to the whole vector at once.\n",
    "This output becomes the input for the next layer, and the process repeats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1bd4d39a-e9d5-431f-b791-1c1db5de5360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1, 3, 3, 2), (2, 2, 2, 1))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "A convolution is NOT a standard tensor multiplication. Instead, it's a specialized operation where:\n",
    "\n",
    "1. The kernel slides over the input\n",
    "2. At each position, we compute element-wise multiplications followed by summation\n",
    "\n",
    "Kernel Shape Rules!\n",
    "For an input with shape [batch_size, height, width, channels]:\n",
    "\n",
    "- The kernel shape is [kernel_height, kernel_width, in_channels, out_channels]\n",
    "- in_channels MUST match the input's channels dimension\n",
    "- out_channels is a hyperparameter you choose\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Example input - a small feature map/activation (e.g., from a previous layer).\n",
    "# Shape: [batch_size=1, height=3, width=3, channels=2].\n",
    "input_activation = np.array(\n",
    "    [\n",
    "        [\n",
    "            [[0.2, 0.5], [0.1, 0.3], [0.7, 0.2]],\n",
    "            [[0.5, 0.8], [0.4, 0.1], [0.3, 0.9]],\n",
    "            [[0.6, 0.4], [0.8, 0.3], [0.2, 0.5]]],\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 1. AFFINE TRANSFORMATION\n",
    "# For CNNs, this is typically done with convolution kernels (filters).\n",
    "# Example kernel with shape [height=2, width=2, in_channels=2, out_channels=1].\n",
    "kernel = np.array(\n",
    "    [\n",
    "        [\n",
    "            [[0.1], [0.3]],\n",
    "            [[0.5], [0.7]]\n",
    "        ],\n",
    "        [\n",
    "            [[0.2], [0.4]],\n",
    "             [[0.6], [0.8]],\n",
    "        ]\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Bias term for the affine transformation.\n",
    "bias = 0.1\n",
    "\n",
    "input_activation.shape, kernel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fa87321c-b0ab-4553-8ca3-5ccd42484876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k_h=2, k_w=2\n",
      "After affine transformation (convolution):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[2.77],\n",
       "          [3.2 ]],\n",
       " \n",
       "         [[3.59],\n",
       "          [3.35]]]]),\n",
       " (1, 2, 2, 1))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manually perform convolution (simplified for demonstration).\n",
    "# In a real implementation, you'd use functions like torch.nn.Conv2d.\n",
    "# output_height = input_height - kernel_height + 1\n",
    "# output_width = input_width - kernel_width + 1\n",
    "output_height = input_activation.shape[1] - kernel.shape[0] + 1\n",
    "output_width = input_activation.shape[2] - kernel.shape[1] + 1\n",
    "output = np.zeros((1, output_height, output_width, 1))\n",
    "\n",
    "# Perform the convolution (the affine transformation).\n",
    "k_h = kernel.shape[0]\n",
    "k_w = kernel.shape[1]\n",
    "print(f'{k_h=}, {k_w=}')\n",
    "for h in range(output_height):\n",
    "    for w in range(output_width):\n",
    "        # Extract the patch from the input.\n",
    "        patch = input_activation[0, h:h+k_h, w:w+k_w, :]\n",
    "        patch = input_activation[0, h:h+2, w:w+2, :]\n",
    "        \n",
    "        # Element-wise multiplication and sum (dot product).\n",
    "        # This is the affine part: Wx + b where W is the kernel, x is the input, b is bias.\n",
    "        output[0, h, w, 0] = np.sum(patch * kernel) + bias\n",
    "\n",
    "print(\"After affine transformation (convolution):\")\n",
    "output, output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e5423e76-a12b-4921-bd94-73814f216512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After elementwise non-linearity (ReLU):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(array([[[[2.77],\n",
       "          [3.2 ]],\n",
       " \n",
       "         [[3.59],\n",
       "          [3.35]]]]),\n",
       " (1, 2, 2, 1))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. ELEMENTWISE NON-LINEARITY\n",
    "# Apply ReLU (Rectified Linear Unit) non-linearity: f(x) = max(0, x)\n",
    "relu_output = np.maximum(0, output)\n",
    "\n",
    "print(\"After elementwise non-linearity (ReLU):\")\n",
    "relu_output, relu_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3038b0d4-dd61-417a-8060-8d85863d3d71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[0.94103299],\n",
       "          [0.96083428]],\n",
       " \n",
       "         [[0.97314288],\n",
       "          [0.96610484]]]]),\n",
       " (1, 2, 2, 1))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sigmoid_output = 1 / (1 + np.exp(-output))\n",
    "sigmoid_output, sigmoid_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "52357d67-bde7-4215-8675-a43151483d2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[0.99217766],\n",
       "          [0.9966824 ]],\n",
       " \n",
       "         [[0.99847782],\n",
       "          [0.9975412 ]]]]),\n",
       " (1, 2, 2, 1))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tanh_output = np.tanh(output)\n",
    "tanh_output, tanh_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "434a7237-b20a-4253-9617-fa2663f2b7ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[[[2.77],\n",
       "          [3.2 ]],\n",
       " \n",
       "         [[3.59],\n",
       "          [3.35]]]]),\n",
       " (1, 2, 2, 1))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaky_relu_output = np.maximum(0.01 * output, output)\n",
    "leaky_relu_output, leaky_relu_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b3cb8-fa84-418e-83fb-b55e0b3f4433",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Input image: $f(x, y)$\n",
    "Convolution filter: $w(x, y)$\n",
    "\n",
    "$$\n",
    "\\left( f * w \\right) \\left( x, y \\right)\n",
    "=\n",
    "\\sum_{i, j} f\\left( x-i, y-j \\right) \\cdot w(i, j)\n",
    "$$\n",
    "\n",
    "The range of $i$ and $j$ they depend on the filter size.\n",
    "For a filter of size $(2k+1) \\times (2k+1)$, the ranges would typically be:\n",
    "\n",
    "* $i \\in [-k, k]$\n",
    "* $j \\in [-k, k]$\n",
    "\n",
    "For example, with a 3x3 filter (where $k=1$), the indices would range from -1 to 1.\n",
    "\n",
    "Translation operator: $T_{\\Delta x, \\Delta y}f = f(x-\\Delta x, y-\\Delta y)$\n",
    "\n",
    "To prove equivariance we need to show that\n",
    "convolving and then translating gives the same result as translating and then convolving.\n",
    "Mathematically, \n",
    "$T_{\\Delta x, \\Delta y} \\left[ f * w \\right] = T_{\\Delta x, \\Delta y} \\left[ f \\right] * w$.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "T_{\\Delta x, \\Delta y} f * w  &=  (f * w)(x-\\Delta x, y-\\Delta y) \\\\\n",
    "&= \\sum_{i, j} f\\left( \\left(x - \\Delta x\\right) -i, \\left(y - \\Delta y\\right) -j \\right) \\cdot w(i, j) \\\\\n",
    "&= \\sum_{i, j} f\\left( x - \\Delta x -i, y - \\Delta y -j \\right) \\cdot w(i, j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "On the other hand,\n",
    "$$\n",
    "\\begin{align}\n",
    "T_{\\Delta x, \\Delta y} \\left[ f \\right] * w  &= \\sum_{i, j} T_{\\Delta x, \\Delta y}[f]\\left( x - \\Delta x -i, y - \\Delta y -j \\right) \\cdot w(i, j) \\\\\n",
    "&= \\sum_{i, j} f\\left( \\left(x - i\\right) -\\Delta x, \\left(y - j\\right) -\\Delta y \\right) \\cdot w(i, j) \\\\\n",
    "&= \\sum_{i, j} f\\left( x - \\Delta x -i, y - \\Delta y -j \\right) \\cdot w(i, j)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "So we arrived at exactly the same expression, showing us that \n",
    "> \\[convolutions\\] also builds equivariance to translation into the model (in other words, if the image is shifted by one pixel to the right,\n",
    "then the output of the convolution is also shifted one pixel to the right; the two representations vary\n",
    "equally with translation).\n",
    "\n",
    "\n",
    "Though note that CNNs actually do correlations over windows.\n",
    "$$\n",
    "\\left( f * w \\right) \\left( x, y \\right)\n",
    "=\n",
    "\\sum_{i, j} f\\left( x+i, y+j \\right) \\cdot w(i, j)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b44c2e62-0109-423c-814a-ce88faabc8ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Input Matrix (f):\n",
    "[1, 2, 3]\n",
    "[4, 5, 6]\n",
    "[7, 8, 9]\n",
    "\n",
    "Filter (w):\n",
    "[1, 2]\n",
    "[3, 4]\n",
    "'''\n",
    "# Cross-Correlation (what CNNs actually do).\n",
    "'''\n",
    "> Position (0,0): place filter @ this position\n",
    "[1, 2, 3]    [1, 2]\n",
    "[4, 5, 6]    [3, 4]\n",
    "[7, 8, 9]\n",
    "\n",
    "Calculation: place filter @ this position\n",
    "1*1 + 2*2 + 4*3 + 5*4 = 1 + 4 + 12 + 20 = 37\n",
    "'''\n",
    "# (f * w) = sum_{i, j} f(0+i, 0+j) w(i, j)\n",
    "# f(0,0)w(0,0) + f(0,1)w(0,1) + f(1,0)w(1,0) + f(1,1)w(1,1)\n",
    "\n",
    "'''\n",
    "> Position (0, 1):\n",
    "[1, 2, 3]    [1, 2]→\n",
    "[4, 5, 6]    [3, 4]→\n",
    "[7, 8, 9]\n",
    "\n",
    "2*1 + 3*2 + 5*3 + 6*4 = 47\n",
    "'''\n",
    "# (f * w) = sum_{i, j} f(0+i, 1+j) w(i, j)\n",
    "# f(0,1)w(0,0) + f(0,2)w(0,1) + f(1,1)w(1,0) + f(1,2)w(1,1)\n",
    "'''\n",
    "> Position (1,0):\n",
    "[1, 2, 3]    \n",
    "[4, 5, 6]    [1, 2]↓\n",
    "[7, 8, 9]    [3, 4]↓\n",
    "\n",
    "4*1 + 5*2 + 7*3 + 8*4 = 67\n",
    "\n",
    "> Position (1,1):\n",
    "[1, 2, 3]    \n",
    "[4, 5, 6]    [1, 2]↘\n",
    "[7, 8, 9]    [3, 4]↘\n",
    "\n",
    "5*1 + 6*2 + 8*3 + 9*4 = 77\n",
    "\n",
    "Result of cross-correlation:\n",
    "[37, 47]\n",
    "[67, 77]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2d77027-9c12-4992-aebf-5d0f486fb945",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Input Matrix (f):\n",
    "[1, 2, 3]\n",
    "[4, 5, 6]\n",
    "[7, 8, 9]\n",
    "\n",
    "Filter (w):\n",
    "[1, 2]\n",
    "[3, 4]\n",
    "'''\n",
    "# Convolution (mathematical definition)\n",
    "# First, flip the filter horizontally and vertically:\n",
    "'''\n",
    "Flipped filter:\n",
    "[4, 3]\n",
    "[2, 1]\n",
    "'''\n",
    "\n",
    "# Now apply correlation with the flipped filter:\n",
    "\n",
    "'''\n",
    "> Position (0,0): place flipped filter @ this position\n",
    "[1, 2, 3]    [4, 3]\n",
    "[4, 5, 6]    [2, 1]\n",
    "[7, 8, 9]\n",
    "\n",
    "Calculation:\n",
    "1*4 + 2*3 + 4*2 + 5*1 = 4 + 6 + 8 + 5 = 23\n",
    "'''\n",
    "# (f * w) = sum_{i, j} f(0-i, 0-j) w(i, j)\n",
    "# This is equivalent to:\n",
    "# f(0,0)w(0,0) + f(0,-1)w(0,1) + f(-1,0)w(1,0) + f(-1,-1)w(1,1)\n",
    "# With flipped filter, this becomes:\n",
    "# f(0,0)w'(0,0) + f(0,1)w'(0,1) + f(1,0)w'(1,0) + f(1,1)w'(1,1)\n",
    "# where w' is the flipped filter\n",
    "\n",
    "'''\n",
    "> Position (0,1):\n",
    "[1, 2, 3]    [4, 3]→\n",
    "[4, 5, 6]    [2, 1]→\n",
    "[7, 8, 9]\n",
    "\n",
    "Calculation:\n",
    "2*4 + 3*3 + 5*2 + 6*1 = 8 + 9 + 10 + 6 = 33\n",
    "'''\n",
    "# (f * w) = sum_{i, j} f(0-i, 1-j) w(i, j)\n",
    "# With flipped filter, equivalent to cross-correlation at this position\n",
    "\n",
    "'''\n",
    "> Position (1,0):\n",
    "[1, 2, 3]    \n",
    "[4, 5, 6]    [4, 3]↓\n",
    "[7, 8, 9]    [2, 1]↓\n",
    "\n",
    "Calculation:\n",
    "4*4 + 5*3 + 7*2 + 8*1 = 16 + 15 + 14 + 8 = 53\n",
    "\n",
    "> Position (1,1):\n",
    "[1, 2, 3]    \n",
    "[4, 5, 6]    [4, 3]↘\n",
    "[7, 8, 9]    [2, 1]↘\n",
    "\n",
    "Calculation:\n",
    "5*4 + 6*3 + 8*2 + 9*1 = 20 + 18 + 16 + 9 = 63\n",
    "\n",
    "Result of convolution:\n",
    "[23, 33]\n",
    "[53, 63]\n",
    "'''\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f74aefe3-95e9-42b8-b2c1-7af284163be0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
